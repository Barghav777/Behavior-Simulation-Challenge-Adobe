{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KP_LJqfuYf5"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sKk3UHzArZpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv('Your dataset path')"
      ],
      "metadata": {
        "id": "meNRt4jprjwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "Kkjn0WmJtL9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "EINhS2fP8Pil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to hold embeddings\n",
        "x_content = []\n",
        "x_username = []\n",
        "x_company = []\n",
        "x_date = []\n",
        "\n",
        "# Batch size\n",
        "batch_size = 64  # You can adjust this based on your memory capacity\n",
        "num_rows = dataset.shape[0]\n",
        "\n",
        "# Process in batches\n",
        "for i in range(0, num_rows, batch_size):\n",
        "    # Get the current batch\n",
        "    batch_content = dataset['content'][i:i + batch_size].tolist()\n",
        "    batch_username = dataset['username'][i:i + batch_size].tolist()\n",
        "    batch_company = dataset['inferred company'][i:i + batch_size].tolist()\n",
        "    batch_date = dataset['date'][i:i + batch_size].tolist()\n",
        "\n",
        "    # Process the entire batch with spaCy\n",
        "    docs_content = nlp.pipe(batch_content, batch_size=batch_size)\n",
        "    docs_username = nlp.pipe(batch_username, batch_size=batch_size)\n",
        "    docs_company = nlp.pipe(batch_company, batch_size=batch_size)\n",
        "    docs_date = nlp.pipe(batch_date, batch_size=batch_size)\n",
        "\n",
        "    # Extract embeddings for each feature in the batch\n",
        "    for doc in docs_content:\n",
        "        x_content.append(doc.vector)\n",
        "\n",
        "    for doc in docs_username:\n",
        "        x_username.append(doc.vector)\n",
        "\n",
        "    for doc in docs_company:\n",
        "        x_company.append(doc.vector)\n",
        "\n",
        "    for doc in docs_date:\n",
        "        x_date.append(doc.vector)\n",
        "\n",
        "    print(f\"Processed batch {i // batch_size + 1} out of {num_rows // batch_size + 1}\")\n",
        "\n",
        "# Convert lists to NumPy arrays (optional, depending on your use case)\n",
        "x_content = np.array(x_content)\n",
        "x_username = np.array(x_username)\n",
        "x_company = np.array(x_company)\n",
        "x_date = np.array(x_date)\n",
        "\n",
        "# Now you can use x_content, x_username, x_company, and x_date as needed\n"
      ],
      "metadata": {
        "id": "gtg5MvvA8Qun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = pd.concat([pd.DataFrame(x_content), pd.DataFrame(x_username), pd.DataFrame(x_company), pd.DataFrame(x_date)], axis=1)"
      ],
      "metadata": {
        "id": "6eMc1kWLuGfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "cc5CnqaN9f-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "from xgboost import XGBRegressor, XGBClassifier"
      ],
      "metadata": {
        "id": "r0R5xW9auLiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = joblib.load(\"/content/drive/MyDrive/xgboost_model.joblib\")"
      ],
      "metadata": {
        "id": "5XS-kho99im8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(x)\n",
        "y_cat = classifier.predict(x)"
      ],
      "metadata": {
        "id": "uZdiGDV67RMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_reg1 = joblib.load(\"/content/drive/MyDrive/xgb_regressor_model_cat1.joblib\")\n",
        "y_reg2 = joblib.load(\"/content/drive/MyDrive/xgb_regressor_model_cat2.joblib\")\n",
        "y_reg3 = joblib.load(\"/content/drive/MyDrive/xgb_regressor_model_cat3.joblib\")\n",
        "y_reg4 = joblib.load(\"/content/drive/MyDrive/xgb_regressor_model_cat4.joblib\")\n",
        "y_reg5 = joblib.load(\"/content/drive/MyDrive/xgb_regressor_model_cat5.joblib\")\n",
        "y_reg6 = joblib.load(\"/content/drive/MyDrive/xgb_regressor_model_cat6.joblib\")"
      ],
      "metadata": {
        "id": "oSE89imd7lUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = []\n",
        "for i in range(len(dataset)):\n",
        "  tweet.append(dataset['content'][i] + \" \" + dataset['date'][i] + \" \" + dataset['username'][i] + \" \" + dataset[\"inferred company\"][i])"
      ],
      "metadata": {
        "id": "byRqZDXB9DES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['tweet'] = tweet"
      ],
      "metadata": {
        "id": "R-Kj_aeH9mog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bertweet.to(device)\n",
        "bertweet.eval()\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "def get_bertweet_embeddings(sentences, model, tokenizer, device):\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
        "\n",
        "    print(\"Input tensor shape:\", inputs[\"input_ids\"].shape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "start = time.time()\n",
        "print(\"train data embeddings started\")\n",
        "\n",
        "X_test = None\n",
        "\n",
        "for start_idx in range(0, len(dataset), batch_size):\n",
        "    end_idx = min(start_idx + batch_size, len(dataset))\n",
        "    batch_sentences = [text for text in dataset[\"tweet\"].iloc[start_idx:end_idx].tolist() if text]\n",
        "\n",
        "    try:\n",
        "        X_batch = get_bertweet_embeddings(batch_sentences, bertweet, tokenizer, device)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error processing batch {start_idx} to {end_idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "    if start_idx == 0:\n",
        "        X_test = X_batch\n",
        "    else:\n",
        "        X_test = np.concatenate([X_test, X_batch])\n",
        "\n",
        "np.save(\"content-test-company-test_full.npy\", X_test)\n",
        "\n",
        "print(f\"Time required = {time.time() - start}\")\n",
        "print(\"test data embeddings ended\")\n"
      ],
      "metadata": {
        "id": "soD03AL08LkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "for i in range(len(X_test)):\n",
        "  x_reg = np.array(X_test[i]).reshape(1, 768)\n",
        "  if y_cat[i] == 0:\n",
        "      likes = y_reg1.predict(x_reg)\n",
        "  elif y_cat[i] == 1:\n",
        "      likes = y_reg2.predict(x_reg)\n",
        "  elif y_cat[i] == 2:\n",
        "      likes = y_reg3.predict(x_reg)\n",
        "  elif y_cat[i] == 3:\n",
        "      likes = y_reg4.predict(x_reg)\n",
        "  elif y_cat[i] == 4:\n",
        "      likes = y_reg5.predict(x_reg)\n",
        "  else:\n",
        "      likes = y_reg6.predict(x_reg)\n",
        "  y_pred.append(likes)"
      ],
      "metadata": {
        "id": "sTOHkmDM7W_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQ563hW-z_av"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}